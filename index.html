<!-- Copyright: https://pengsida.net/ -->

<!doctype html>
<html>

<head>
<title>Dian Zheng</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Dian Zheng, Incoming The Chinese University of Hong Kong"> 
<meta name="description" content="Dian Zheng's homepage">
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel='stylesheet' href='https://chinese-fonts-cdn.deno.dev/packages/xuandongkaishu/dist/XuandongKaishu/result.css' />
<link rel="shortcut icon" href="images/yougis.png" type="image/x-icon">

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-137722442-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        // var e = document.getElementById(id);
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }
</script>

<!-- é¼ æ ‡æ‚¬åœæ˜¾ç¤ºå›¾ç‰‡ -->
<style>
    /* å®¹å™¨æ ·å¼ */
    .hover-container {
      position: relative;
      display: inline-block;
    }
  
    /* é»˜è®¤éšè—å›¾ç‰‡ */
    .hover-image {
        display: none;
        position: absolute;
        bottom: 100%;  /* å›¾ç‰‡æ˜¾ç¤ºåœ¨æ‚¬åœå…ƒç´ ä¸Šæ–¹ */
        left: 125%;
        transform: translateX(-50%);
        z-index: 999;
        width: 200px;  /* è‡ªå®šä¹‰å›¾ç‰‡å®½åº¦ */
        border: 2px solid #ccc;
    }
  
    /* æ‚¬åœæ—¶æ˜¾ç¤ºå›¾ç‰‡ */
    .hover-trigger:hover + .hover-image {
      display: block;
    }
  </style>

</head>

<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Dian Zheng <chinese_name>éƒ‘å…¸</chinese_name> <h1>
				</div>
          <h3 class="title">Building wonderful Content Creator with MLLMs and diffusion</h3>
				<p>
          Incoming Ph.D. student, MMLab@CUHK, <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a></br>
          <div class="hover-container">
              Email: <a href="mailto:zd1423606603@gmail.com">zd1423606603@gmail.com</a> <b>|</b>
              <span class="hover-trigger">WeChat: zd1423606603</span>
              <img src="images/wechat_QRCode.jpg" class="hover-image" alt="æç¤ºå›¾ç‰‡">
          </div> </br>
					</br>
          [<a href="https://scholar.google.com/citations?user=0dkD_dcAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>][<a href="https://github.com/zhengdian1" target="_blank">Github</a>][<a href="https://x.com/Mike_Dian_Zheng" target="_blank">X (Twitter)</a>]</br>
        </p>

			</td>
			<td width="35%">
				<img src="images/new.jpg" width="100%" style="border-radius: 10%;"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>Biography</h2> 

<div style="display: flex">
  <p><chinese_name>I'm passionately obsessed with novelty, but at the same time, ideas are cheap, show me your code.</chinese_name></br></p>
</div>

<div style="display: flex">
    <p>
      I am an incoming PHD student at <a href="https://www.cuhk.edu.hk/chinese/index.html">CUHK</a>, MMLab, advised by <a
      href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a>. Before that, I obtain my master degree at <a href="https://www.sysu.edu.cn/sysuen/">SYSU</a> in 2025, advised by <a
      href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a> and B.E. degree at <a href="https://www.dlut.edu.cn/en/">DLUT</a> in 2022.
    </p>
</div>

<div style="display: flex; margin-bottom: -10px">
    <p>
       ğŸ› ï¸ Employment: <br>
        <!-- [2025/06â€”Present] Research Intern, Multi-modal Foundation Models for Humanoid Robots at <a href="https://lightrobo.com/" target="_blank">Light Robotics</a>. <br> -->
        [2024/12â€”2025/06] Research Intern, Shanghai AI Laboratory, honored to be advised by <a href="https://liuziwei7.github.io/">Prof. Ziwei Liu</a>.  <br>
        [2024/05â€”2024/11] Research Intern, Alibaba, Mentor: Cao Li.
    </p>
</div>

<p>
  I am always open to research collaborations about generative model, robotics. Feel free to contact me. 
</p>

<h2>News</h2>

<style>
  .scrollable-list {
    max-height: 200px; /* æ§åˆ¶æ˜¾ç¤ºåŒºåŸŸçš„é«˜åº¦ */
    overflow-y: scroll; /* å‚ç›´æ»šåŠ¨æ¡ */
    padding: 10px;

    /* å®Œå…¨é€æ˜èƒŒæ™¯ */
    background-color: transparent;
    border: none; /* ç§»é™¤è¾¹æ¡† */
  }

  /* è‡ªå®šä¹‰æ»šåŠ¨æ¡æ ·å¼ï¼ˆä»…æ”¯æŒ Webkit æµè§ˆå™¨ï¼Œå¦‚ Chromeï¼‰ */
  .scrollable-list::-webkit-scrollbar {
    width: 8px;
  }

  .scrollable-list::-webkit-scrollbar-thumb {
    background-color: #888; /* æ»šåŠ¨æ¡æ»‘å—é¢œè‰² */
    border-radius: 4px;
  }

  .scrollable-list::-webkit-scrollbar-track {
    background-color: #f1f1f1; /* æ»šåŠ¨æ¡è½¨é“é¢œè‰² */
    border-radius: 4px;
  }

  /* Firefox æ»šåŠ¨æ¡æ ·å¼ */
  .scrollable-list {
    scrollbar-width: auto;
    scrollbar-color: #888 #f1f1f1;
  }

  .scrollable-list li {
    margin-bottom: 10px;
  }
</style>

<ul class="scrollable-list">
  <li>
    <div>[06/2025] Honored to secure Outstanding Master's Thesis in Sun Yat-sen University!</div>
  </li>
  <li>
    <div>[05/2025] Honored to be selected as Outstanding Graduate in Sun Yat-Sen University!</div>
  </li>
  <li>
    <div>[04/2025] PanoDecouple and DELETE are selected as CVPR2025 <span style="font-weight: bold; color: red;">Highlight</span> !</div>
  </li>
  <li>
    <div>[03/2025] VBench-2.0 is released! Test the boundaries of your methods!</div>
  </li>
  <li>
    <div>[02/2025] 3 papers are accepted by CVPR2025 (3/4, 75% success rate).</div>
  </li>
  <li>
    <div>[01/2025] DiffuVolume has finally been accepted by IJCV2025 after a long wait!</div>
  </li>
  <li>
    <div>[12/2024] Awarded Xiaomi Grand Prize Scholarship of Sun Yat-Sen University.</div>
  </li>
  <li>
    <div>[11/2024] Rated as a Top Reviewer of NeurIPS 2024.</div>
  </li>
  <li>
    <div>[11/2024] SpatialDreamer is released, I would call it the most advanced spatial video generation technology in the industry. Stay tuned!</div>
  </li>
  <li>
    <div>[07/2024] 1 paper is accepted by ECCV 2024.</div>
  </li>
  <li>
    <div>[02/2024] 2 papers are accepted by CVPR 2024.</div>
  </li>
  <li>
    <div>[07/2023] 1 paper is accepted by ICCV 2023.</div>
  </li>
  <li>
    <div>[02/2023] 1 paper is accepted by CVPR 2023.</div>
  </li>
  <li>
    <div>[05/2022] 1 paper is accepted by ICASSP 2022.</div>
  </li>
</ul>

<h2>
    Publications
    <span style="font-size: 65%;">(* denotes equal contribution, and <span class="corresponding">â€ </span> denotes the corresponding author)</span>
</h2>

<div class="newline_bg">
    <h3>First, Co-First, Last Author</h3>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/vbench2.jpg">
  </div>
  <div class="publication_title">
    <p>
      VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness</br>
      <b>Dian Zheng*</b>, Ziqi Huang*, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng<span class="corresponding">â€ </span>, Yu Qiao<span class="corresponding">â€ </span>, Ziwei Liu<span class="corresponding">â€ </span></br>
      Arxiv </br>
      [<a href="https://arxiv.org/abs/2503.21755" target="_blank">ArXiv</a>]
      [<a href="https://github.com/Vchitect/VBench/tree/master/VBench-2.0" target="_blank">Code</a>]
      [<a href="https://vchitect.github.io/VBench-2.0-project/" target="_blank">Project Page</a>]
    </p>
  </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/PanoDecouple.jpg">
    </div>
    <div class="publication_title">
      <p>
        Panorama Generation From NFoV Image Done Right</br>
        <b>Dian Zheng</b>, Cheng Zhang, Xiao-Ming Wu, Cao Li<span class="corresponding">â€ </span>, Chengfei Lv, Jian-Fang Hu, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
        IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025 <b><span style="color:#FF7777";>ğŸ†ï¸ Highlight</span></b></br>
        [<a href="https://arxiv.org/abs/2503.18420" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/PanoDecouple" target="_blank">Code</a>][<a href="https://isee-laboratory.github.io/PanoDecouple/" target="_blank">Project Page</a>]
      </p>
    </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/delete.jpg">
  </div>
  <div class="publication_title">
    <p>
      Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks</br>
      Yu Zhou*, <b>Dian Zheng*</b>, Qijie Mo, Ren-Jie Lu, Kun-Yu Lin<span class="corresponding">â€ </span>, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025 <b><span style="color:#FF7777";>ğŸ†ï¸ Highlight</span></b></br>
      [<a href="https://arxiv.org/abs/2503.23751" target="_blank">Paper</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/SpatialDreamer.jpg">
  </div>
  <div class="publication_title">
    <p>
      SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input</br>
      Zhen Lv, Yangqi Long, Congzhentao Huang, Cao Li, Chengfei Lv<span class="corresponding">â€ </span>, Hao Ren, <b>Dian Zheng</b></br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025</br>
      [<a href="https://arxiv.org/abs/2411.11934" target="_blank">Paper</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/diffvolume.jpg">
  </div>
  <div class="publication_title">
    <p>
      Diffusion Model for Volume based Stereo Matching</br>
      <b>Dian Zheng</b>, Xiao-Ming Wu, Zuhao Liu, Jingke Meng, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      International Journal of Computer Vision (<b>IJCV</b>), 2025 </br>
      [<a href="https://rdcu.be/d8jDz" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/DiffuVolume" target="_blank">Code</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/diffuir.jpg">
  </div>
  <div class="publication_title">
    <p>
      Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</br>
      <b>Dian Zheng</b>, Xiao-Ming Wu, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024 </br>
      [<a href="https://arxiv.org/abs/2403.11157" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">Code</a>]
    </p>
  </div>
</div>

<div class="newline_bg">
    <h3>Others</h3>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/economic.jpg">
    </div>
    <div class="publication_title">
      <p>
        An Economic Framework for 6-DoF Grasp Detection</br>
        Xiao-Ming Wu*, Jia-Feng Cai*, Jian-Jian Jiang, <b>Dian Zheng</b>, Yi-Lin Wei, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
        European Conference on Computer Vision (<b>ECCV</b>), 2024</br>
        [<a href="https://arxiv.org/abs/2407.08366" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">Code</a>]
      </p>
    </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/dgtr.jpg">
  </div>
  <div class="publication_title">
    <p>
      Dexterous Grasp Transformer</br>
      Guo-Hao Xu*, Yi-Lin Wei*, <b>Dian Zheng</b>, Xiao-Ming Wu, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</br>
      [<a href="https://arxiv.org/abs/2404.18135" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">Code</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/reste.jpg">
  </div>
  <div class="publication_title">
    <p>
      Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</br>
      Xiao-Ming Wu, <b>Dian Zheng</b>, Zuhao Liu, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</br>
      [<a href="https://arxiv.org/abs/2308.06689" target="_blank">Paper</a>][<a href="https://github.com/iSEE-Laboratory/ReSTE" target="_blank">Code</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/pfmf.jpg">
  </div>
  <div class="publication_title">
    <p>
      Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping</br>
      Zuhao Liu, Xiao-Ming Wu, <b>Dian Zheng</b>, Kun-Yu Lin, Wei-Shi Zheng<span class="corresponding">â€ </span></br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023</br>
      [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf" target="_blank">Paper</a>]
    </p>
  </div>
</div>

<div class="publication_container">
  <div class="publication_image">
      <img src="images/uwst.jpg">
  </div>
  <div class="publication_title">
    <p>
      Underwater Stereo Matching Via Unsupervised Appearance And Feature Adaptation Networks</br>
      Zhong Wei, Yazhi Yuan, Xinchen Ye<span class="corresponding">â€ </span>, <b>Dian Zheng</b>, Rui Xu</br>
      International Conference on Acoustics, Speech, and Signal Processing (<b>ICASSP</b>) 2022</br>
      [<a href="https://ieeexplore.ieee.org/abstract/document/9746701" target="_blank">Paper</a>]
    </p>
  </div>
</div>

<h2>
  Academic Service
</h2>

<div style="display: flex">
  <p>
    Conference Reviewer: NeurIPS2025, ACMMM2025, ICME2025, ICML2025, CVPR2025, ICLR2025, NeurIPS2024 (Top Reviewer), PRCV2023</br>
    Journal Reviewer: TCSVT, PR, CMV</br>
  </p>
</div>

<h2>
  Awards
</h2>

<div style="display: flex">
  <p>
    Outstanding Master's Thesis in Sun Yat-sen University (ä¸­å±±å¤§å­¦ä¼˜ç§€ç¡•å£«å­¦ä½è®ºæ–‡), 2025.</br>
    Outstanding Graduate in Sun Yat-Sen University (ä¸­å±±å¤§å­¦ä¼˜ç§€æ¯•ä¸šç”Ÿ), 2025.</br>
    Xiaomi Grand Prize Scholarship of Sun Yat-Sen University (ä¸­å±±å¤§å­¦å°ç±³ç‰¹ç­‰å¥–å­¦é‡‘), 2024.</br>
    NeurIPS2024 Top Reviewer, 2024.</br>
    First Prize, Academic Scholarship of Sun Yat-Sen University for Graduate Student (ä¸­å±±å¤§å­¦ç¡•å£«ç ”ç©¶ç”Ÿä¸€ç­‰å¥–åŠ©é‡‘), 2022, 2023, 2024.
  </p>
</div>

</body>
</html>