<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dian Zheng</title>

  <meta name="author" content="Dian Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel='stylesheet' href='https://chinese-fonts-cdn.deno.dev/packages/xuandongkaishu/dist/XuandongKaishu/result.css' />
  <link rel="shortcut icon" href="images/yougis.png" type="image/x-icon">
</head>

<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Dian Zheng </name> <chinese_name>郑典</chinese_name> 
                  </p>
                  <p><chinese_name>I'm passionately obsessed with novelty, but at the same time, ideas are cheap, show me your code.</chinese_name></p>
                  <p>I am currently a third-year master's student at <a href="https://www.sysu.edu.cn/sysuen/">Sun
                      Yat-sen University</a> (SYSU), where I am advised by <a
                      href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng</a>.
                    Before coming to SYSU, I obtained my B.E. degree at the <a href="https://www.dlut.edu.cn/en/">
                      Dalian University of Technology</a>. 
                  </p>
                  <p>
                      I'm fortunate to have internships at Alibaba and Shanghai AI Lab and I am honored to be advised by <a
                      href="https://liuziwei7.github.io/">Ziwei Liu</a>.
                  </p>
                  <p>
                    My main research focuses on AIGC and its applications on downstream tasks.
                    In the following years, I will focus on exploring the potential of unifying visual generation and understanding, unleashing the power of both and making a step to the true world simulator. 
                      I am always open to research discussions and collaborations. 
                  </p>
                  <p style="text-align:center">
                    <a href="zhengd35@mail2.sysu.edu.cn">Email</a> &nbsp;/
                    <a href="https://scholar.google.com/citations?user=0dkD_dcAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp;/&nbsp;&nbsp;
                    <a href="https://github.com/zhengdian1">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhengdian.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhengdian_cycle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                </td>
              </tr>
              
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [04/2025] PanoDecouple and DELETE are selected as CVPR2025 <span style="font-weight: bold; color: red;">Highlight</span> !
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [03/2025] VBench-2.0 is released! Test the boundaries of your methods!
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [02/2025] 3 papers are accepted by CVPR2025 (3/4, 75% success rate).
                </td>
              </tr>
              
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [01/2025] DiffuVolume has finally been accepted by IJCV2025 after a long wait!
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [12/2024] Awarded Xiaomi Grand Prize Scholarship of Sun Yat-Sen University.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [11/2024] Ended a wonderful 6-month internship at Alibaba.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [11/2024] SpatialDreamer is released, I would call it the most advanced spatial video generation technology in the industry. Stay tuned!
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [11/2024] Rated as a Top Reviewer of NeurIPS 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [07/2024] 1 paper is accepted by ECCV 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [02/2024] 2 papers are accepted by CVPR 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [07/2023] 1 paper is accepted by ICCV 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [02/2023] 1 paper is accepted by CVPR 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [05/2022] 1 paper is accepted by ICASSP 2022.
                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                    Below are my publications. (& means equal contribution, * refers to corresponding author.)
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/vbench2.jpg' width="250"></div>
                </td>
                <td width="75%" valign="center">
                  <papertitle>VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness </papertitle>
                  <br>
                  <strong>Dian Zheng&</strong>,
                  <a href="https://ziqihuangg.github.io">Ziqi Huang&</a>,
                  <a href="https://github.com/Alexios-hub">Hongbo Liu</a>,
                  <a href="https://github.com/Jacky-hate">Kai Zou</a>,
                  <a href="https://github.com/yinanhe">Yinan He</a>,
                  <a href="https://github.com/zhangfan-p">Fan Zhang</a>,
                  <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=GUxrycUAAAAJ&hl=zh-CN">Jingwen He</a>,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                  <a href="http://mmlab.siat.ac.cn/yuqiao/index.html">Yu Qiao*</a>,
                  <a href="https://liuziwei7.github.io/">Ziwei Liu*</a>,
                  <br>
                  <em>ArXiv</em>
                  <br>
                  <a href="https://arxiv.org/abs/2503.21755">[ArXiv]</a>
                  <a href="https://github.com/Vchitect/VBench/tree/master/VBench-2.0">[Code]</a>
                  <a href="https://vchitect.github.io/VBench-2.0-project/">[Project Page]</a>
                  <br>
                  <strong>(Video Generation Evaluation)</strong> Beyond Superficial Faithfulness, to target on next generation of video generation models, it's time to shift to evaluate the Intrinsic Faithfulness in the generated video.
                  We comprehensively evaluate the latest video generation models in 18 dimensions. We hope it will be helpful to the development of video generation.
                </td>
              </tr> <!--zheng2025vbench-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/PanoDecouple.jpg' width="250"></div>
                </td>
                <td width="75%" valign="center">
                  <papertitle>Panorama Generation From NFoV Image Done Right </papertitle>
                  <br>
                  <strong>Dian Zheng</strong>,
                  <a href="https://chengzhag.github.io/">Cheng Zhang</a>,
                  <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                  Cao Li*,
                  Chengfei Lv,
                  <a href="https://isee-ai.cn/~hujianfang/">Jian-Fang Hu</a>,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025 <span style="font-weight: bold; color: red;">(Highlight)</span>
                  <br>
                  <a href="https://arxiv.org/abs/2503.18420">[ArXiv]</a>
                  <a href="https://github.com/iSEE-Laboratory/PanoDecouple">[Code]</a>
                  <a href="https://isee-laboratory.github.io/PanoDecouple/">[Project Page]</a>
                  <br>
                  <strong>(Panorama Generation)</strong> A novel decoupled paradigm to generate accurate distortion and appealing visual results at the same time.
                    With our model, you can freely utilize real-world image with random NFoV, and even freely perform text-to-panorama generation. Enjoy it!
                </td>
              </tr> <!--zheng2025panorama-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/delete.jpg' width="250"></div>
                </td>
                <td width="75%" valign="center">
                  <papertitle>Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks</papertitle>
                  <br>
                  Yu Zhou&,
                  <strong>Dian Zheng&</strong>,
                  <a href="https://scholar.google.com/citations?user=1p5raSkAAAAJ&hl=zh-CN">Qijie Mo</a>,
                  <a href="https://lu-renjie.github.io/">Renjie Lu</a>,
                  <a href="https://kunyulin.github.io/">Kun-Yu Lin*</a>,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025 <span style="font-weight: bold; color: red;">(Highlight)</span>
                  <br>
                  <a href="https://arxiv.org/abs/2503.23751">[ArXiv]</a>
                  <br>
                  <strong>(Any Class-centric Unlearning)</strong> A general unlearning solution for any Class-centric tasks, which can achieve state-of-the-art performance without using retained data and pretrained model knowledge.
                </td>
              </tr> <!--zhou2025decoupled-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/diffvolume.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Diffusion Model for Volume based Stereo Matching</papertitle>
                    <br>
                    <strong>Dian Zheng</strong>,
                    <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=0ee541wAAAAJ&hl=zh-CN&oi=ao">Jingke Meng</a>,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>
                    <br>
                    <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2025
                    <br>
                    <a href="https://rdcu.be/d8jDz">[ArXiv]</a>
                    <a href="https://github.com/iSEE-Laboratory/DiffuVolume">[Code]</a>
                  <br>
                  <strong>(Stereo Matching)</strong> A lightweight, plug-and-play diffusion filter, boosting your stereo matching methods easily!
                </td>
              </tr> <!--zheng2023diffuvolume-->

              <tr>
                <!-- <td style="padding:20px;width:35%;max-width:35%" align="center"> -->
                <td style="padding:20px;width:35%" align="center">
                  <video style="max-width: 100%; max-height: 300px;" muted autoplay loop>
                    <source src="images/SpatialDreamer.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </td>
                <td width="75%" valign="center">
                  <papertitle>SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input </papertitle>
                  <br>
                  <a
                    href="https://scholar.google.com.hk/citations?hl=zh-CN&user=x38N1hcAAAAJ&view_op=list_works&sortby=pubdate">Zhen
                    Lv</a>,
                  <a href="https://scholar.google.com/citations?user=9YX-jRMAAAAJ&hl=zh-CN&oi=ao">Yangqi Long</a>,
                  Congzhentao Huang,
                  Cao Li,
                  Chengfei Lv*,
                  Hao Ren,
                  <strong>Dian Zheng</strong>,
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2411.11934">[ArXiv]</a>
                  <br>
                  <strong>(Spatial Video Generation)</strong> A novel self-supervised stereo video generation paradigm
                    that does not require paired stereo video data. I would call it the most advanced technology in the industry!
                </td>
              </tr> <!--lv2024spatialdreamer-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/economic.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>An Economic Framework for 6-DoF Grasp Detection </papertitle>
                    <br>
                    <a href="https://dravenalg.github.io/">Xiao-Ming Wu&</a>,
                    Jia-Feng Cai&,
                    Jian-Jian Jiang,
                    <strong>Dian Zheng</strong>,
                    <a href="https://github.com/wyl2077">Yi-Lin Wei</a>,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                    <br>
                    <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2407.08366">[ArXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/EconomicGrasp">[Code]</a>
                  <br>
                  <strong>(6-DoF Grasping)</strong> Speed up your 6-DoF grasping training 10x without performance drop!
                </td>
              </tr> <!--wu2024economicgrasp-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/diffuir.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</papertitle>
                    <br>
                    <strong>Dian Zheng</strong>,
                    <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                    <a href="https://scholar.google.com/citations?user=y-gvXecAAAAJ&hl=zh-CN&oi=ao">Shuzhou Yang</a>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=7brFI_4AAAAJ">Jian Zhang</a>,
                    <a href="https://isee-ai.cn/~hujianfang/">Jian-Fang Hu</a>,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2403.11157">[ArXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/DiffUIR">[Code]</a>
                  <br>
                  <strong>(Universal Image Restoration)</strong> Boosting the development of Universal Image Restoration by ajusting the diffusion algorithm!
                </td>
              </tr> <!--zheng2024selective-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/dgtr.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Dexterous Grasp Transformer</a> </papertitle>
                    <br>
                    <a href="https://github.com/KwokhoTsui">Guo-Hao Xu&</a>,
                    <a href="https://github.com/wyl2077">Yi-Lin Wei&</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>,
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2404.18135">[ArXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/DGTR">[Code]</a>
                  <br>
                  <strong>(Dexterous Grasping)</strong> The first discriminative framework for generating a diverse, high quality set of feasible dexterous grasp only in one pass!
                </td>
              </tr> <!--xu2024dexterous-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/reste.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</papertitle>
                    <br>
                    <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                    <strong>Dian Zheng</strong>,
                    Zuhao Liu,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>
                    <br>
                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
                    <br>
                    <a href="https://arxiv.org/abs/2308.06689">[ArXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/ReSTE">[Code]</a>
                  <br>
                  <strong>(Binary Neural Networks)</strong> Proposing root-based STE for binary neural network training, achieving SOTA without bells and whistles!
                </td>
              </tr> <!--wu2023estimator-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/pfmf.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping </papertitle>
                    <br>
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=QMm29SwAAAAJ&hl=en&oi=ao">Xiao-Ming Wu</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>,
                    <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng*</a>
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
                    <br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf">[ArXiv]</a>
                  <br>
                  <strong>(Video Anomaly Detection)</strong> The first syn2real anomaly feature generator. 
                </td>
              </tr> <!--liu2023generating-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/uwst.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <papertitle>Underwater Stereo Matching Via Unsupervised Appearance And Feature Adaptation Networks </papertitle>
                    <br>
                    Zhong Wei,
                    Yazhi Yuan,
                    <a href="https://scholar.google.com/citations?user=iZ3YxM0AAAAJ&hl=zh-CN">Xinchen Ye*</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=rgtOrCMAAAAJ">Rui Xu</a>
                    <br>
                    <em>International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>)</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9746701">[ArXiv]</a>
                  <br>
                </td>
              </tr> <!--zhong2022underwater-->

            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Academic Service</h2>
                  <p>
                    Conference Reviewer: NeurIPS2025, ACMMM2025, ICCV2025, ICME2025, ICML2025, CVPR2025, ICLR2025, NeurIPS2024 (Top Reviewer), PRCV2023
                  </p>
                  <p>
                    Journal Reviewer: PR, CMV
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Awards</h2>
                  <p>
                    First Prize, Academic Scholarship of Sun Yat-Sen University for Graduate Student (中山大学硕士研究生一等奖助金),
                    2022, 2023, 2024.
                  </p>
                  <p>
                    Xiaomi Grand Prize Scholarship of Sun Yat-Sen University (中山大学小米特等奖学金), 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="right border=" 0" cellpadding="20">
            <tbody>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Last Update 12/25/2024. Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon
                      Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
