<!DOCTYPE HTML>
<!-- <style>
  #full {
    display: none;
  }
</style> -->
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dian Zheng</title>

  <meta name="author" content="Dian Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel='stylesheet' href='https://chinese-fonts-cdn.deno.dev/packages/xuandongkaishu/dist/XuandongKaishu/result.css' />
  <link rel="shortcut icon" href="images/yougis.png" type="image/x-icon">
</head>

<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Dian Zheng </name> <chinese_name>郑典</chinese_name>
                  </p>
                  <p>I am currently a third-year master's student at <a href="https://www.sysu.edu.cn/sysuen/">Sun
                      Yat-sen University</a> (SYSU), where I am advised by <a
                      href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>.
                    Before coming to SYSU, I obtained my B.E. degree at the <a href="https://www.dlut.edu.cn/en/">
                      Dalian University of Technology</a>. I'm fortunate to have internships at Alibaba.
                  </p>
                  <p>
                    My research interests mainly lie around Computer Vision, with the goal of developing general and
                    generalizable vision systems.
                    My main research focuses on AIGC and its applications on downstream tasks.
                    I believe it is one of the essential tools for facilitating people's interaction with modern vision
                    systems and enhancing the quality of their lives.
                    Now, I am committing to take a small step toward this vision.
                  </p>
                  <p>
                    <span class="highlight">In the following years, I will focus on 2d/3d vision AIGC and efficient
                      AIGC. And I am always open to research discussions and collaborations. </span>
                  </p>
                  <p style="text-align:center">
                    <a href="zhengd35@mail2.sysu.edu.cn">Email</a> &nbsp;/
                    <a href="https://scholar.google.com/citations?user=0dkD_dcAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp;/&nbsp;&nbsp;
                    <a href="https://github.com/zhengdian1">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/zhengdian.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/zhengdian_cycle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [01/2025] DiffuVolume has finally been accepted by IJCV2025 after a long wait！
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [12/2024] Awarded Xiaomi Grand Prize Scholarship of Sun Yat-Sen University.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [11/2024] Ended a wonderful 6-month internship at Alibaba.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [11/2024] Rated as a Top Reviewer of NeurIPS 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [07/2024] 1 paper is accepted by ECCV 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [02/2024] 2 papers are accepted by CVPR 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [08/2023] 1 paper is submitted to IJCV.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [07/2023] 1 paper is accepted by ICCV 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [02/2023] 1 paper is accepted by CVPR 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                  [05/2022] 1 paper is accepted by ICASSP 2022.
                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <!-- <td style="padding:20px;width:35%;max-width:35%" align="center"> -->
                <td style="padding:20px;width:35%" align="center">
                  <video style="max-width: 100%; max-height: 300px;" muted autoplay loop>
                    <source src="images/SpatialDreamer.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </td>
                <td width="75%" valign="center">
                  <papertitle>SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input </papertitle>
                  <br>
                  <a
                    href="https://scholar.google.com.hk/citations?hl=zh-CN&user=x38N1hcAAAAJ&view_op=list_works&sortby=pubdate">Zhen
                    Lv</a>,
                  <a href="https://scholar.google.com/citations?user=9YX-jRMAAAAJ&hl=zh-CN&oi=ao">Yangqi Long</a>,
                  Congzhentao Huang,
                  Cao Li,
                  Chengfei Lv*,
                  Hao Ren,
                  <strong>Dian Zheng</strong>,
                  <br>
                  <em>arxiv</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2411.11934">[arXiv]</a>
                  <br>
                  <p> In this paper, we propose SpatialDreamer, a novel self-supervised stereo video generation paradigm
                    that does not require paired stereo video data. The key insight is to apply the forward-backward
                    rendering to data building. </p>
                </td>
              </tr> <!--lv2024spatialdreamer-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/economic.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>An Economic Framework for 6-DoF Grasp Detection </papertitle>
                    <br>
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu&</a>,
                    Jia-Feng Cai&,
                    Jian-Jian Jiang,
                    <strong>Dian Zheng</strong>,
                    <a href="https://github.com/wyl2077">Yi-Lin Wei</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
                    <br>
                    <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2407.08366">[arXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/EconomicGrasp">[code]</a>
                  <br>
                </td>
              </tr> <!--wu2024economicgrasp-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/diffuir.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</papertitle>
                    <br>
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    <a href="https://scholar.google.com/citations?user=y-gvXecAAAAJ&hl=zh-CN&oi=ao">Shuzhou Yang</a>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=7brFI_4AAAAJ">Jian Zhang</a>,
                    <a href="https://scholar.google.com.sg/citations?user=4WsBaB4AAAAJ&hl=en">Jian-Fang Hu</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2403.11157">[arXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/DiffUIR">[code]</a>
                  <br>
                </td>
              </tr> <!--zheng2024selective-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/dgtr.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Dexterous Grasp Transformer</a> </papertitle>
                    <br>
                    <a href="https://github.com/KwokhoTsui">Guo-Hao Xu&</a>,
                    <a href="https://github.com/wyl2077">Yi-Lin Wei&</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                    <br>
                    <a href="https://arxiv.org/abs/2404.18135">[arXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/DGTR">[code]</a>
                  <br>
                </td>
              </tr> <!--xu2024dexterous-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/diffvolume.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Diffusion Model for Volume based Stereo Matching</papertitle>
                    <br>
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=0ee541wAAAAJ&hl=zh-CN&oi=ao">Jingke Meng</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                    <br>
                    <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2025
                    <br>
                    <a href="https://trebuchet.public.springernature.app/get_content/d8b3d3d9-f68c-49ef-b728-d81d5f2185ba?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20250201&utm_content=10.1007/s11263-025-02362-1">[arXiv]</a>
                    <a href="https://github.com/iSEE-Laboratory/DiffuVolume">[code]</a>
                  <br>
                </td>
              </tr> <!--zheng2023diffuvolume-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/reste.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</papertitle>
                    <br>
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    <strong>Dian Zheng</strong>,
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                    <br>
                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
                    <br>
                    <a href="https://arxiv.org/abs/2308.06689">[arXiv]</a> 
                    <a href="https://github.com/iSEE-Laboratory/ReSTE">[code]</a>
                  <br>
                </td>
              </tr> <!--wu2023estimator-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/pfmf.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                    <papertitle>Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping </papertitle>
                    <br>
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=QMm29SwAAAAJ&hl=en&oi=ao">Xiao-Ming Wu</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
                    <br>
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf">[arXiv]</a>
                  <br>
                </td>
              </tr> <!--liu2023generating-->

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/uwst.jpg' width="250"></div>
                </td>
                <td width="75%" valign="middle">
                  <p>
                    <papertitle>Underwater Stereo Matching Via Unsupervised Appearance And Feature Adaptation Networks </papertitle>
                    <br>
                    Zhong Wei,
                    Yazhi Yuan,
                    <a href="https://scholar.google.com/citations?user=iZ3YxM0AAAAJ&hl=zh-CN">Xinchen Ye*</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=rgtOrCMAAAAJ">Rui Xu</a>
                    <br>
                    <em>International Conference on Acoustics, Speech, and Signal Processing (<strong>ICASSP</strong>)</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9746701">[arXiv]</a>
                  <br>
                </td>
              </tr> <!--zhong2022underwater-->

            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Academic Service</h2>
                  <p>
                    Conference Reviewer: ICME2025, ICML2025, CVPR2025, ICLR2025, NeurIPS2024 (Top Reviewer), PRCV2023
                  </p>
                  <p>
                    Journal Reviewer: PR, CMV
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Awards</h2>
                  <p>
                    First Prize, Academic Scholarship of Sun Yat-Sen University for Graduate Student (中山大学硕士研究生一等奖助金),
                    2022, 2023, 2024.
                  </p>
                  <p>
                    Xiaomi Grand Prize Scholarship of Sun Yat-Sen University (中山大学小米特等奖学金), 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="right border=" 0" cellpadding="20">
            <tbody>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Last Update 12/25/2024. Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon
                      Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>