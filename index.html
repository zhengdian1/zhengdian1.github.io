<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dian Zheng</title>

    <meta name="author" content="Dian Zheng">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                <name>Dian Zheng | ÈÉëÂÖ∏</name>
              </p>
                <p>I am currently a third-year master's student at <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a> (SYSU), where I am advised by <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>.
                  Before coming to SYSU, I obtained my B.E. degree at the <a href="https://www.dlut.edu.cn/en/"> Dalian University of Technology</a>. I'm fortunate to have internships at Alibaba.
                </p>
                <p>
                  My research interests mainly lie around Computer Vision, with the goal of developing general and generalizable vision systems.
                  My main research focuses on AIGC and its applications on downstream tasks.
                  I believe it is one of the essential tools for facilitating people's interaction with modern vision systems and enhancing the quality of their lives.
                  Now, I am committing to take a small step toward this vision.
                </p>
                <p>
                  <span class="highlight">In the following years, I will focus on 2d/3d vision AIGC and efficient AIGC. And I am always open to research discussions and collaborations. </span>
                </p>
                <p style="text-align:center">
                  <a href="zhengd35@mail2.sysu.edu.cn">Email</a> &nbsp;/
                  <a href="https://scholar.google.com/citations?user=0dkD_dcAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;&nbsp;
                  <a href="https://github.com/zhengdian1">Github</a>
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jonbarron/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/zhengdian.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zhengdian_cycle.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [01/2025] DiffuVolume has finally been accepted by IJCV2025 after a long waitÔºÅ
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [12/2024] Awarded Xiaomi Grand Prize Scholarship of Sun Yat-Sen University.
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [11/2024] Ended a wonderful 6-month internship at Alibaba.
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [11/2024] Rated as a Top Reviewer of NeurIPS 2024.
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [07/2024] 1 paper is accepted by ECCV 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [02/2024] 2 papers are accepted by CVPR 2024.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [08/2023] 1 paper is submitted to IJCV.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [07/2023] 1 paper is accepted by ICCV 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [02/2023] 1 paper is accepted by CVPR 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [05/2022] 1 paper is accepted by ICASSP 2022.
                </td>
              </tr>

          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SpatialDreamer.jpg" alt="dise">
            </td>
          <td width="75%" valign="middle">
              <p>
                  <papertitle>SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input </papertitle>
              <br>
                  <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=x38N1hcAAAAJ&view_op=list_works&sortby=pubdate">Zhen Lv</a>,
                  <a href="https://scholar.google.com/citations?user=9YX-jRMAAAAJ&hl=zh-CN&oi=ao">Yangqi Long</a>,
                  Congzhentao Huang,
                  Cao Li,
                  Chengfei Lv*,
                  Hao Ren,
                  <strong>Dian Zheng</strong>,
                <br>
                <em>arxiv</em>, 2024
                <br>
                </p>
                <div class="paper" id="lv2024spatialdreamer">
                    <a href="https://arxiv.org/abs/2411.11934">[arXiv]</a> 
                </div>
                <br>
            <p> In this paper, we propose SpatialDreamer, a novel self-supervised stereo video generation paradigm that does not require paired stereo video data. The key insight is to apply the forward-backward rendering to data building. </p>
            </td>
        </tr> <!--lv2024spatialdreamer-->
      
            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/economic.jpg' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                      <papertitle><a href="https://arxiv.org/abs/2407.08366">An Economic Framework for 6-DoF Grasp Detection</a> </papertitle>
<!--                       <papertitle>An Economic Framework for 6-DoF Grasp Detection</a> </papertitle> -->
                  <br>
                      <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu&</a>,
                      Jia-Feng Cai&,
                      Jian-Jian Jiang,
                      <strong>Dian Zheng</strong>,
                      <a href="https://github.com/wyl2077">Yi-Lin Wei</a>,
                      <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  </p>
                  <div class="paper" id="wu2024economicgrasp">
                      <a href="https://arxiv.org/abs/2407.08366">paper</a> /
                      <a href="https://github.com/iSEE-Laboratory/EconomicGrasp">code</a>
                  </div>
                  <br>
              </td>
          </tr> <!--wu2024economicgrasp-->
              
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/diffuir.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/abs/2403.11157">Selective Hourglass Mapping for Universal Image Restoration Based on
                      Diffusion Model</a> </papertitle>
                <br>
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    <a href="https://scholar.google.com/citations?user=y-gvXecAAAAJ&hl=zh-CN&oi=ao">Shuzhou Yang</a>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=7brFI_4AAAAJ">Jian Zhang</a>,
                    <a href="https://scholar.google.com.sg/citations?user=4WsBaB4AAAAJ&hl=en">Jian-Fang Hu</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
                <br>
                <em>CVPR</em>, 2024
                <br>
                </p>
                <div class="paper" id="zheng2024selective">
                    <a href="https://arxiv.org/abs/2403.11157">paper</a> /
                    <a href="https://github.com/iSEE-Laboratory/DiffUIR">code</a>
                </div>
                <br>
            </td>
        </tr> <!--zheng2024selective-->

        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='images/dgtr.jpg' width="250"></div>
                  </td>
          <td width="75%" valign="middle">
              <p>
                  <!-- <papertitle><a href="https://arxiv.org/abs/2311.01734">Dexterous Grasp Transformer</a> </papertitle> -->
                  <papertitle>Dexterous Grasp Transformer</a> </papertitle>
              <br>
                  <a href="https://github.com/KwokhoTsui">Guo-Hao Xu&</a>,
                  <a href="https://github.com/wyl2077">Yi-Lin Wei&</a>,
                  <strong>Dian Zheng</strong>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                  <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>,
              <br>
              <em>CVPR</em>, 2024
              <br>
              </p>
              <div class="paper" id="xu2024dexterous">
                  <a href="https://arxiv.org/abs/2404.18135">paper</a> /
                  <a href="https://github.com/iSEE-Laboratory/DGTR">code</a>
              </div>
              <br>
          </td>
      </tr> <!--xu2024dexterous-->
              
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/diffvolume.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://trebuchet.public.springernature.app/get_content/d8b3d3d9-f68c-49ef-b728-d81d5f2185ba?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20250201&utm_content=10.1007/s11263-025-02362-1">DiffuVolume: Diffusion Model for Volume based Stereo
                      Matching</a></papertitle>
                <br>
                <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=0ee541wAAAAJ&hl=zh-CN&oi=ao">Jingke Meng</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                <br>
                <em>IJCV</em>, 2025
                <br>
                </p>
                <div class="paper" id="zheng2023diffuvolume">
                    <a href="https://trebuchet.public.springernature.app/get_content/d8b3d3d9-f68c-49ef-b728-d81d5f2185ba?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20250201&utm_content=10.1007/s11263-025-02362-1">paper</a> /
                    <a href="https://github.com/iSEE-Laboratory/DiffuVolume">code</a>
                </div>
                <br>
            </td>
            </tr> <!--zheng2023diffuvolume-->
      
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/reste.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/abs/2308.06689">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</a></papertitle>
                <br>
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ">Xiao-Ming Wu</a>,
                <strong>Dian Zheng</strong>,
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                </p>
                <div class="paper" id="wu2023estimator">
                    <a href="https://arxiv.org/abs/2308.06689">paper</a> /
                    <a href="https://github.com/iSEE-Laboratory/ReSTE">code</a>
                </div>
                <br>
            </td>
            </tr> <!--wu2023estimator-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/pfmf.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf">Generating Anomalies for Video Anomaly Detection with
                      Prompt-based Feature Mapping</a> </papertitle>
                <br>
                    Zuhao Liu,
                    <a href="https://scholar.google.com/citations?user=QMm29SwAAAAJ&hl=en&oi=ao">Xiao-Ming Wu</a>,
                    <strong>Dian Zheng</strong>,
                    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng*</a>
                <br>
                <em>CVPR</em>, 2023
                <br>
                </p>
                <div class="paper" id="liu2023generating">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf">paper</a> 
                </div>
                <br>
            </td>
        </tr> <!--liu2023generating-->

      <tr>
        <td style="padding:20px;width:35%;vertical-align:middle">
            <img src='images/uwst.jpg' width="250"></div>
                </td>
        <td width="75%" valign="middle">
            <p>
                <papertitle><a href="https://ieeexplore.ieee.org/abstract/document/9746701">UNDERWATER STEREO MATCHING VIA UNSUPERVISED APPEARANCE AND FEATURE
                  ADAPTATION NETWORKS</a> </papertitle>
            <br>
                Zhong Wei,
                Yazhi Yuan,
                <a href="https://scholar.google.com/citations?user=iZ3YxM0AAAAJ&hl=zh-CN">Xinchen Ye*</a>,
                <strong>Dian Zheng</strong>,
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=rgtOrCMAAAAJ">Rui Xu</a>
            <br>
            <em>ICASSP</em>, 2022
            <br>
            </p>
            <div class="paper" id="zhong2022underwater">
                <a href="https://ieeexplore.ieee.org/abstract/document/9746701">paper</a> 
            </div>
            <br>
        </td>
    </tr> <!--zhong2022underwater-->

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Academic Service</h2>
                  <p>
                      Conference Reviewer: ICME2025, ICML2025, CVPR2025, ICLR2025, NeurIPS2024 (Top Reviewer), PRCV2023
                  </p>
                  <p>
                      Journal Reviewer: PR, CMV
                  </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
                  <p>
                      First Prize, Academic Scholarship of Sun Yat-Sen University for Graduate Student (‰∏≠Â±±Â§ßÂ≠¶Á°ïÂ£´Á†îÁ©∂Áîü‰∏ÄÁ≠âÂ•ñÂä©Èáë), 2022, 2023, 2024.
                  </p>
                  <p>
                      Xiaomi Grand Prize Scholarship of Sun Yat-Sen University (‰∏≠Â±±Â§ßÂ≠¶Â∞èÁ±≥ÁâπÁ≠âÂ•ñÂ≠¶Èáë), 2024.
                  </p>
              </td>
            </tr>
          </tbody></table>
    
          <table width="100%" align="right border="0" cellpadding="20"><tbody>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last Update 12/25/2024. Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
